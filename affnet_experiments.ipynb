{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/home/leon/mishkin_affiliated/affnet/examples/hesaffnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('affnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from past import autotranslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import seaborn as sns\n",
    "import os\n",
    "import errno\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import gc\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "from Utils import L2Norm, cv2_scale\n",
    "#from Utils import np_reshape64 as np_reshape\n",
    "np_reshape = lambda x: np.reshape(x, (64, 64, 1))\n",
    "from Utils import str2bool\n",
    "autotranslate(['dataset'])\n",
    "from dataset import HPatchesDM,TripletPhotoTour, TotalDatasetsLoader\n",
    "cv2_scale40 = lambda x: cv2.resize(x, dsize=(40, 40),\n",
    "                                 interpolation=cv2.INTER_LINEAR)\n",
    "from augmentation import get_random_norm_affine_LAFs,get_random_rotation_LAFs, get_random_shifts_LAFs\n",
    "from LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs\n",
    "from pytorch_sift import SIFTNet\n",
    "from HardNet import HardNet, L2Norm, HardTFeatNet\n",
    "from Losses import loss_HardNegC, loss_HardNet\n",
    "from SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\n",
    "from LAF import denormalizeLAFs, LAFs2ell, abc2A,visualize_LAFs\n",
    "from Losses import distance_matrix_vector\n",
    "from ReprojectionStuff import get_GT_correspondence_indexes\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:242: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "  nn.init.orthogonal(m.weight.data, gain=0.8)\n",
      "affnet/architectures.py:244: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias.data, 0.01)\n"
     ]
    }
   ],
   "source": [
    "#This is only siple example. It DOES NOT DO state-of-art image matching, not even close, because it is lacking RANSAC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\n",
    "from LAF import denormalizeLAFs, LAFs2ell, abc2A\n",
    "from Utils import line_prepender\n",
    "from architectures import AffNetFast\n",
    "from HardNet import HardNet\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "### Initialization\n",
    "AffNetPix = AffNetFast(PS = 32)\n",
    "weightd_fname = 'affnet/pretrained/AffNet.pth'\n",
    "\n",
    "checkpoint = torch.load(weightd_fname)\n",
    "AffNetPix.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "AffNetPix.eval()\n",
    "    \n",
    "detector = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = 3000,\n",
    "                                          border = 5, num_Baum_iters = 1, \n",
    "                                          AffNet = AffNetPix)\n",
    "descriptor = HardNet()\n",
    "model_weights = 'affnet/HardNet++.pth'\n",
    "hncheckpoint = torch.load(model_weights)\n",
    "descriptor.load_state_dict(hncheckpoint['state_dict'])\n",
    "descriptor.eval()\n",
    "if USE_CUDA:\n",
    "    detector = detector.cuda()\n",
    "    descriptor = descriptor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSampleDatasetsLoader(data.Dataset):\n",
    "    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 5000000, fliprot = False, *arg, **kw):\n",
    "        super(OneSampleDatasetsLoader, self).__init__()\n",
    "        datasets_path = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path)]\n",
    "        start = True\n",
    "        for dataset_p in datasets_path:\n",
    "            d = torch.load(dataset_p)\n",
    "            if start:\n",
    "                data = d[0]\n",
    "                labels = d[1]\n",
    "                start = False\n",
    "            else:\n",
    "                data = torch.cat([data, d[0]])\n",
    "                labels = torch.cat([labels, d[1]+ torch.max(labels) + 1])\n",
    "        #datasets = [torch.load(dataset) for dataset in datasets_path]\n",
    "        #data, labels = datasets[0][0], datasets[0][1]\n",
    "        #\n",
    "        #for i in range(1,len(datasets)):\n",
    "        #    data = torch.cat([data,datasets[i][0]])\n",
    "        #    labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n",
    "        #\n",
    "        #del datasets\n",
    "        self.data, self.labels = data, labels\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.n_triplets = n_triplets\n",
    "        self.batch_size = batch_size\n",
    "        self.fliprot = fliprot\n",
    "        if self.train:\n",
    "                print('Generating {} triplets'.format(self.n_triplets))\n",
    "                self.pairs = self.generate(self.labels, self.n_triplets, self.batch_size)\n",
    "\n",
    "\n",
    "    def generate(self, labels, num_triplets, batch_size):\n",
    "            def create_indices(_labels):\n",
    "                inds = dict()\n",
    "                for idx, ind in enumerate(_labels):\n",
    "                    if ind not in inds:\n",
    "                        inds[ind] = []\n",
    "                    inds[ind].append(idx)\n",
    "                return inds\n",
    "            triplets = []\n",
    "            indices = create_indices(labels.numpy())\n",
    "            unique_labels = np.unique(labels.numpy())\n",
    "            n_classes = unique_labels.shape[0]\n",
    "            # add only unique indices in batch\n",
    "            already_idxs = set()\n",
    "            for x in tqdm(range(num_triplets)):\n",
    "                if len(already_idxs) >= batch_size:\n",
    "                    already_idxs = set()\n",
    "                c1 = unique_labels[np.random.randint(0, n_classes)]\n",
    "                while c1 in already_idxs:\n",
    "                    c1 = unique_labels[np.random.randint(0, n_classes)]\n",
    "                already_idxs.add(c1)\n",
    "                try:\n",
    "                    y = indices[c1]\n",
    "                except:\n",
    "                    print(indices.keys())\n",
    "                    sys.exit(0)\n",
    "                if len(indices[c1]) == 2:  # hack to speed up process\n",
    "                    n1, n2 = 0, 1\n",
    "                else:\n",
    "                    n1 = np.random.randint(0, len(indices[c1]))\n",
    "#                     n2 = np.random.randint(0, len(indices[c1]))\n",
    "#                     while n1 == n2:\n",
    "#                         n2 = np.random.randint(0, len(indices[c1]))\n",
    "                triplets.append(indices[c1][n1])\n",
    "            return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            def transform_img(img):\n",
    "                if self.transform is not None:\n",
    "                    img = (img.numpy())/255.0\n",
    "                    img = self.transform(img)\n",
    "                return img\n",
    "\n",
    "            t = self.pairs[index]\n",
    "            a = self.data[t]\n",
    "            img_a = transform_img(a)\n",
    "\n",
    "            # transform images if required\n",
    "            if self.fliprot:\n",
    "                do_flip = random.random() > 0.5\n",
    "                do_rot = random.random() > 0.5\n",
    "\n",
    "                if do_rot:\n",
    "                    img_a = img_a.permute(0,2,1)\n",
    "\n",
    "                if do_flip:\n",
    "                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n",
    "            return img_a\n",
    "\n",
    "    def __len__(self):\n",
    "            if self.train:\n",
    "                return self.pairs.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pairs = 1000000#0\n",
    "b_size = 16\n",
    "\n",
    "# def create_loaders():\n",
    "\n",
    "#     kwargs = {'num_workers': 8, 'pin_memory': True}\n",
    "#     transform = transforms.Compose([\n",
    "#             transforms.Lambda(np_reshape),\n",
    "#             transforms.ToTensor()\n",
    "#             ])\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#             TotalDatasetsLoader(\n",
    "#                             datasets_path = 'affnet/dataset/6Brown', #datasets_path = args.dataroot,\n",
    "#                             train=True,\n",
    "#                              n_triplets = n_pairs, #n_triplets = 10000000,\n",
    "#                              fliprot=True,\n",
    "#                              batch_size=b_size,\n",
    "#                              download=True,\n",
    "#                              transform=transform),\n",
    "#                              batch_size=b_size,\n",
    "#                              shuffle=False, **kwargs)\n",
    "#     return train_loader, None\n",
    "\n",
    "def create_loaders():\n",
    "\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True}\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            OneSampleDatasetsLoader(\n",
    "                            datasets_path = 'affnet/dataset/6Brown', #datasets_path = args.dataroot,\n",
    "                            train=True,\n",
    "                             n_triplets = n_pairs, #n_triplets = 10000000,\n",
    "                             fliprot=True,\n",
    "                             batch_size=b_size,\n",
    "                             download=True,\n",
    "                             transform=transform),\n",
    "                             batch_size=b_size,\n",
    "                             shuffle=False, **kwargs)\n",
    "    return train_loader, None\n",
    "\n",
    "def extract_and_crop_patches_by_predicted_transform(patches, trans, crop_size = 32):\n",
    "    assert patches.size(0) == trans.size(0)\n",
    "    st = int((patches.size(2) - crop_size) / 2)\n",
    "    fin = st + crop_size\n",
    "    rot_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n",
    "    if patches.is_cuda:\n",
    "        rot_LAFs = rot_LAFs.cuda()\n",
    "        trans = trans.cuda()\n",
    "    rot_LAFs1  = torch.cat([torch.bmm(trans, rot_LAFs[:,0:2,0:2]), rot_LAFs[:,0:2,2:]], dim = 2);\n",
    "    return extract_patches(patches,  rot_LAFs1, PS = patches.size(2))[:,:, st:fin, st:fin].contiguous()\n",
    "    \n",
    "    \n",
    "def extract_and_crop_patches_by_predicted_transform_featuremap(patches, trans, crop_size = 32):\n",
    "    assert patches.size(0) == trans.size(0)\n",
    "    st = int((patches.size(2) - crop_size) / 2)\n",
    "    fin = st + crop_size\n",
    "    rot_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n",
    "    if patches.is_cuda:\n",
    "        rot_LAFs = rot_LAFs.cuda()\n",
    "        trans = trans.cuda()\n",
    "    rot_LAFs1  = torch.cat([torch.bmm(trans, rot_LAFs[:,0:2,0:2]), rot_LAFs[:,0:2,2:]], dim = 2);\n",
    "    return extract_patches(patches,  rot_LAFs1, PS = patches.size(2))[:,:, st:fin, st:fin].contiguous()\n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_random_LAF(data, max_rot = math.pi, max_tilt = 1.0, crop_size = 32):\n",
    "    st = int((data.size(2) - crop_size)/2)\n",
    "    fin = st + crop_size\n",
    "    if type(max_rot) is float:\n",
    "        rot_LAFs, inv_rotmat = get_random_rotation_LAFs(data, max_rot)\n",
    "    else:\n",
    "        rot_LAFs = max_rot\n",
    "        inv_rotmat = None\n",
    "    aff_LAFs, inv_TA = get_random_norm_affine_LAFs(data, max_tilt);\n",
    "    aff_LAFs[:,0:2,0:2] = torch.bmm(rot_LAFs[:,0:2,0:2],aff_LAFs[:,0:2,0:2])\n",
    "    data_aff = extract_patches(data,  aff_LAFs, PS = data.size(2))\n",
    "    data_affcrop = data_aff[:,:, st:fin, st:fin].contiguous()\n",
    "    return data_affcrop, data_aff, rot_LAFs,inv_rotmat,inv_TA\n",
    "\n",
    "def load_grayscale_var(fname):\n",
    "    img = Image.open(fname).convert('RGB')\n",
    "    img = np.mean(np.array(img), axis = 2)\n",
    "    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n",
    "    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n",
    "    if args.cuda:\n",
    "        var_image_reshape = var_image_reshape.cuda()\n",
    "    return var_image_reshape\n",
    "def get_geometry_and_descriptors(img, det, desc, do_ori = True):\n",
    "    with torch.no_grad():\n",
    "        LAFs, resp = det(img,do_ori = do_ori)\n",
    "        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n",
    "        descriptors = desc(patches)\n",
    "    return LAFs, descriptors\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr=0.0001):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] = lr * (\n",
    "        1.0 - float(group['step']) * float(b_size) / (n_pairs * float(20)))\n",
    "    return\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                          momentum=0.9, dampening=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.orthogonal(m.weight.data, gain=0.6)\n",
    "        try:\n",
    "            nn.init.constant(m.bias.data, 0.01)\n",
    "        except:\n",
    "            pass\n",
    "    return\n",
    "\n",
    "class HardNetPart(nn.Module):\n",
    "    \"\"\"HardNetPart model definition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(HardNetPart, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "        )\n",
    "        self.features.apply(weights_init)\n",
    "        return\n",
    "    \n",
    "    def input_norm(self,x):\n",
    "        flat = x.view(x.size(0), -1)\n",
    "        mp = torch.mean(flat, dim=1)\n",
    "        sp = torch.std(flat, dim=1) + 1e-7\n",
    "        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x_features = self.features(self.input_norm(input))\n",
    "        x = x_features.view(x_features.size(0), -1)\n",
    "        return L2Norm()(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:03<00:00, 275532.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = create_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS = 32\n",
    "# tilt_schedule = {'0': 3.0, '1': 4.0, '3': 4.5, '5': 4.8, '6': 5.2, '8':  5.8 }\n",
    "\n",
    "# loss_f = MSELoss()\n",
    "# LOG_DIR = './logs'\n",
    "# def train(train_loader,test_loader, affnet, descriptor, hardnet_part, optimizer, epoch):\n",
    "#     hardnet_part.train()\n",
    "#     descriptor.eval()\n",
    "#     dense_descriptor = nn.Sequential(*list(descriptor.features.children())[:-6])\n",
    "#     pbar = tqdm(enumerate(train_loader))\n",
    "#     for batch_idx, data in pbar:\n",
    "# #         data_a, data_p = data\n",
    "# #         data_a, data_p  = data_a.float().cuda(), data_p.float().cuda()\n",
    "        \n",
    "#         data_a, data_p = data\n",
    "#         data_a = data_a.float().cuda()\n",
    "        \n",
    "        \n",
    "#         st = int((data_p.size(2) - affnet.PS)/2)\n",
    "        \n",
    "        \n",
    "#         #fin = st + model.PS\n",
    "#         ep1 = epoch\n",
    "#         while str(ep1) not in tilt_schedule.keys():\n",
    "#             ep1 -=1\n",
    "#             if ep1 < 0:\n",
    "#                 break\n",
    "#         max_tilt = tilt_schedule[str(ep1)]\n",
    "        \n",
    "#         data_a_aff_crop, data_a_aff, rot_LAFs_a, inv_rotmat_a, inv_TA_a = extract_random_LAF(data_a, math.pi, max_tilt, affnet.PS)\n",
    "        \n",
    "#         #data_p_aff_crop, data_p_aff, rot_LAFs_p, inv_rotmat_p, inv_TA_p = extract_random_LAF(data_p, rot_LAFs_a, max_tilt, affnet.PS)\n",
    "        \n",
    "# #         if inv_rotmat_p is None:\n",
    "# #             inv_rotmat_p = inv_rotmat_a\n",
    "        \n",
    "        \n",
    "#         #out_a_aff, out_p_aff = affnet(data_a_aff_crop,True), affnet(data_p_aff_crop,True)\n",
    "        \n",
    "#         out_a_aff = affnet(data_a_aff_crop,True)\n",
    "#         #out_a_aff_back = torch.bmm(torch.bmm(out_a_aff, inv_TA_a),  inv_rotmat_a)\n",
    "#         #out_p_aff_back = torch.bmm(torch.bmm(out_p_aff, inv_TA_p),  inv_rotmat_p)\n",
    "#         ###### Get descriptors\n",
    "        \n",
    "#         out_patches_a_crop = extract_and_crop_patches_by_predicted_transform(data_a_aff, out_a_aff, crop_size = affnet.PS)\n",
    "#         #out_patches_p_crop = extract_and_crop_patches_by_predicted_transform(data_p_aff, out_p_aff, crop_size = affnet.PS)\n",
    "        \n",
    "#         desc_features = dense_descriptor(data_a_aff_crop).detach()\n",
    "        \n",
    "#         basic_descriptor = descriptor(out_patches_a_crop).detach()\n",
    "        \n",
    "#         desc_a = descriptor(data_a_aff_crop)\n",
    "        \n",
    "#         desc_a = desc_a.detach()\n",
    "#         #desc_p = descriptor(out_patches_p_crop)\n",
    "        \n",
    "        \n",
    "#         #descr_dist =  torch.sqrt(((desc_a - desc_p)**2).view(data_a.size(0),-1).sum(dim=1) + 1e-6).mean()\n",
    "        \n",
    "#         #geom_dist = torch.sqrt(((out_a_aff_back - out_p_aff_back)**2 ).view(-1,4).sum(dim=1) + 1e-8).mean()\n",
    "        \n",
    "# #         if args.loss == 'HardNet':\n",
    "# #             loss = loss_HardNet(desc_a,desc_p); \n",
    "# #         elif args.loss == 'HardNegC':\n",
    "# #             loss = loss_HardNegC(desc_a,desc_p); \n",
    "# #         #elif args.loss == 'Geom':\n",
    "# #         #    loss = geom_dist; \n",
    "# #         elif args.loss == 'PosDist':\n",
    "# #             loss = descr_dist; \n",
    "# #         else:\n",
    "# #             print('Unknown loss function')\n",
    "# #             sys.exit(1)\n",
    "\n",
    "#         print(out_a_aff.shape)\n",
    "#         break\n",
    "#         desc_features = extract_and_crop_patches_by_predicted_transform(desc_features, out_a_aff, crop_size = desc_features.shape[0])    \n",
    "#         approxim = hardnet_part(desc_features)\n",
    "        \n",
    "#         loss = loss_f(approxim, desc_a)    \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         adjust_learning_rate(optimizer)\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             pbar.set_description(\n",
    "#                 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "#                     epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "#                            100. * batch_idx / len(train_loader),\n",
    "#                     float(loss.detach().cpu().numpy())))\n",
    "#     torch.save({'epoch': epoch + 1, 'state_dict': hardnet_part.state_dict()},\n",
    "#                '{}/checkpoint_{}.pth'.format(LOG_DIR,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from LAF import denormalizeLAFs\n",
    "\n",
    "PS = 32\n",
    "tilt_schedule = {'0': 3.0, '1': 4.0, '3': 4.5, '5': 4.8, '6': 5.2, '8':  5.8 }\n",
    "\n",
    "loss_f = MSELoss()\n",
    "LOG_DIR = './logs'\n",
    "\n",
    "\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    \n",
    "def train(train_loader,test_loader, detector, descriptor, hardnet_part, optimizer, epoch):\n",
    "    hardnet_part.train()\n",
    "    descriptor.eval()\n",
    "    \n",
    "    dense_descriptor = nn.Sequential(*list(descriptor.features.children())[:-6])\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    for batch_idx, data_a in pbar:\n",
    "#         data_a, data_p = data\n",
    "#         data_a, data_p  = data_a.float().cuda(), data_p.float().cuda()\n",
    "        \n",
    "        #data_a, data_p = data\n",
    "        data_a = data_a.float().cuda()\n",
    "        \n",
    "        \n",
    "        st = int((data_a.size(2) - PS)/2)\n",
    "        \n",
    "        \n",
    "        #fin = st + model.PS\n",
    "        ep1 = epoch\n",
    "        while str(ep1) not in tilt_schedule.keys():\n",
    "            ep1 -=1\n",
    "            if ep1 < 0:\n",
    "                break\n",
    "        max_tilt = tilt_schedule[str(ep1)]\n",
    "        \n",
    "        #data_a_aff_crop, data_a_aff, rot_LAFs_a, inv_rotmat_a, inv_TA_a = extract_random_LAF(data_a, math.pi, max_tilt, PS*2)\n",
    "        \n",
    "        #data_p_aff_crop, data_p_aff, rot_LAFs_p, inv_rotmat_p, inv_TA_p = extract_random_LAF(data_p, rot_LAFs_a, max_tilt, affnet.PS)\n",
    "        \n",
    "#         if inv_rotmat_p is None:\n",
    "#             inv_rotmat_p = inv_rotmat_a\n",
    "        \n",
    "        \n",
    "        #out_a_aff, out_p_aff = affnet(data_a_aff_crop,True), affnet(data_p_aff_crop,True)\n",
    "        \n",
    "        #out_a_aff = affnet(data_a_aff_crop,True)\n",
    "\n",
    "        #print(data_a_aff_crop[0].unsqueeze_(0).shape)\n",
    "        \n",
    "        #print(get_geometry_and_descriptors(data_a_aff_crop[0].unsqueeze_(0), detector, descriptor))\n",
    "        #geom_desc = [get_geometry_and_descriptors(d.unsqueeze_(0), detector, descriptor) for d in data_a_aff_crop]\n",
    "        \n",
    "        \n",
    "        #out_a_aff, basic_descriptor = get_geometry_and_descriptors(data_a_aff_crop, detector, descriptor)\n",
    "#         for d in data_a_aff_crop:\n",
    "#             print(d.shape)\n",
    "        #print(detector.AffNet(data_a).shape)\n",
    "#        for image in data_a:\n",
    "#             with HiddenPrints():\n",
    "#                     LAFs, resp = detector(image.unsqueeze_(0), do_ori = True)\n",
    "#                     patches = detector.extract_patches_from_pyr(LAFs, PS = PS)\n",
    "            \n",
    "            #out_a_aff = detector.AffNet(image)\n",
    "#             print(LAFs.shape)\n",
    "#             print(patches.squeeze().shape)\n",
    "            \n",
    "        patches, data_a_aff, rot_LAFs_a, inv_rotmat_a, inv_TA_a = extract_random_LAF(data_a, math.pi, max_tilt, PS)\n",
    "\n",
    "        #print(data_a_aff.shape)\n",
    "        transforms = detector.AffNet(data_a_aff)\n",
    "        transformed_patches =  extract_and_crop_patches_by_predicted_transform_featuremap(data_a_aff, transforms, crop_size = PS)  \n",
    "\n",
    "        desc_features = dense_descriptor(patches).detach()\n",
    "\n",
    "        desc_a = descriptor(patches)\n",
    "\n",
    "        desc_a = desc_a.detach()\n",
    "\n",
    "#         grid_original_image  = F.affine_grid(out_a_aff, patches.size())\n",
    "#         transformed_patches = F.grid_sample(patches, grid_original_image)\n",
    "        transformed_descriptors = descriptor(transformed_patches).detach()\n",
    "\n",
    "#             grid  = F.affine_grid(out_a_aff, desc_features.size())\n",
    "#             desc_features = F.grid_sample(desc_features, grid)\n",
    "       \n",
    "#         print('transforms',transforms.shape)\n",
    "#         print('data_a_aff',data_a_aff.shape)\n",
    "#         print('patches',patches.shape)\n",
    "#         print('transformed_patches',transformed_patches.shape)\n",
    "#         print('desc_features',desc_features.shape)\n",
    "        transforms_padding = Variable(torch.zeros(transforms.shape[0],transforms.shape[1],1)).cuda()\n",
    "        transforms = torch.cat((transforms,transforms_padding),2)\n",
    "        #print('transforms_padding',transforms_padding.shape)\n",
    "        #desc_features = extract_and_crop_patches_by_predicted_transform_featuremap(desc_features, transforms, crop_size = desc_features.size(0))    \n",
    "        grid  = F.affine_grid(transforms, desc_features.size())\n",
    "        desc_features = F.grid_sample(desc_features, grid)\n",
    "        #print('desc_features',desc_features.shape)\n",
    "        \n",
    "#         print(hardnet_part(desc_features).shape)\n",
    "        #print(desc_a.shape)\n",
    "        approxim = hardnet_part(desc_features)+desc_a\n",
    "\n",
    "        loss = loss_f(approxim, transformed_descriptors)    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        adjust_learning_rate(optimizer)\n",
    "        if batch_idx % 10 == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader),\n",
    "                    float(loss.detach().cpu().numpy())))\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': hardnet_part.state_dict()},\n",
    "               '{}/checkpoint_{}.pth'.format(LOG_DIR,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "Train Epoch: 0 [0/1000000 (0%)]\tLoss: 0.0127: : 0it [00:00, ?it/s]\u001b[A\n",
      "Train Epoch: 0 [0/1000000 (0%)]\tLoss: 0.0127: : 9it [00:00, 85.86it/s]\u001b[A\n",
      "Train Epoch: 0 [160/1000000 (0%)]\tLoss: 0.0126: : 9it [00:00, 69.95it/s]\u001b[A\n",
      "Train Epoch: 0 [160/1000000 (0%)]\tLoss: 0.0126: : 18it [00:00, 87.62it/s]\u001b[A\n",
      "Train Epoch: 0 [320/1000000 (0%)]\tLoss: 0.0134: : 18it [00:00, 75.17it/s]\u001b[A\n",
      "Train Epoch: 0 [320/1000000 (0%)]\tLoss: 0.0134: : 28it [00:00, 88.62it/s]\u001b[A\n",
      "Train Epoch: 0 [480/1000000 (0%)]\tLoss: 0.0118: : 28it [00:00, 79.52it/s]\u001b[A\n",
      "Train Epoch: 0 [480/1000000 (0%)]\tLoss: 0.0118: : 37it [00:00, 88.94it/s]\u001b[A\n",
      "Train Epoch: 0 [640/1000000 (0%)]\tLoss: 0.0148: : 37it [00:00, 79.86it/s]\u001b[A\n",
      "Train Epoch: 0 [640/1000000 (0%)]\tLoss: 0.0148: : 46it [00:00, 88.91it/s]\u001b[A\n",
      "Train Epoch: 0 [800/1000000 (0%)]\tLoss: 0.0127: : 46it [00:00, 80.71it/s]\u001b[A\n",
      "Train Epoch: 0 [800/1000000 (0%)]\tLoss: 0.0127: : 56it [00:00, 90.23it/s]\u001b[A\n",
      "Train Epoch: 0 [960/1000000 (0%)]\tLoss: 0.0141: : 56it [00:00, 83.39it/s]\u001b[A\n",
      "Train Epoch: 0 [960/1000000 (0%)]\tLoss: 0.0141: : 66it [00:00, 91.48it/s]\u001b[A\n",
      "Train Epoch: 0 [1120/1000000 (0%)]\tLoss: 0.0134: : 66it [00:00, 85.26it/s]\u001b[A\n",
      "Train Epoch: 0 [1120/1000000 (0%)]\tLoss: 0.0134: : 76it [00:00, 92.28it/s]\u001b[A\n",
      "Train Epoch: 0 [1280/1000000 (0%)]\tLoss: 0.0120: : 76it [00:00, 86.82it/s]\u001b[A\n",
      "Train Epoch: 0 [1280/1000000 (0%)]\tLoss: 0.0120: : 86it [00:00, 92.96it/s]\u001b[A\n",
      "Train Epoch: 0 [1440/1000000 (0%)]\tLoss: 0.0127: : 86it [00:00, 88.04it/s]\u001b[A\n",
      "Train Epoch: 0 [1440/1000000 (0%)]\tLoss: 0.0127: : 96it [00:01, 93.60it/s]\u001b[A\n",
      "Train Epoch: 0 [1600/1000000 (0%)]\tLoss: 0.0131: : 96it [00:01, 89.06it/s]\u001b[A\n",
      "Train Epoch: 0 [1600/1000000 (0%)]\tLoss: 0.0131: : 106it [00:01, 94.01it/s]\u001b[A\n",
      "Train Epoch: 0 [1760/1000000 (0%)]\tLoss: 0.0120: : 106it [00:01, 89.87it/s]\u001b[A\n",
      "Train Epoch: 0 [1760/1000000 (0%)]\tLoss: 0.0120: : 116it [00:01, 94.36it/s]\u001b[A\n",
      "Train Epoch: 0 [1920/1000000 (0%)]\tLoss: 0.0122: : 116it [00:01, 90.62it/s]\u001b[A\n",
      "Train Epoch: 0 [1920/1000000 (0%)]\tLoss: 0.0122: : 127it [00:01, 94.88it/s]\u001b[A\n",
      "Train Epoch: 0 [2080/1000000 (0%)]\tLoss: 0.0144: : 127it [00:01, 91.91it/s]\u001b[A\n",
      "Train Epoch: 0 [2080/1000000 (0%)]\tLoss: 0.0144: : 137it [00:01, 94.86it/s]\u001b[A\n",
      "Train Epoch: 0 [2240/1000000 (0%)]\tLoss: 0.0122: : 137it [00:01, 92.04it/s]\u001b[A\n",
      "Train Epoch: 0 [2240/1000000 (0%)]\tLoss: 0.0122: : 147it [00:01, 94.78it/s]\u001b[A\n",
      "Train Epoch: 0 [2400/1000000 (0%)]\tLoss: 0.0127: : 147it [00:01, 92.18it/s]\u001b[A\n",
      "Train Epoch: 0 [2400/1000000 (0%)]\tLoss: 0.0127: : 157it [00:01, 94.73it/s]\u001b[A\n",
      "Train Epoch: 0 [2560/1000000 (0%)]\tLoss: 0.0136: : 157it [00:01, 92.38it/s]\u001b[A\n",
      "Train Epoch: 0 [2560/1000000 (0%)]\tLoss: 0.0136: : 167it [00:01, 95.00it/s]\u001b[A\n",
      "Train Epoch: 0 [2720/1000000 (0%)]\tLoss: 0.0130: : 167it [00:01, 92.76it/s]\u001b[A\n",
      "Train Epoch: 0 [2720/1000000 (0%)]\tLoss: 0.0130: : 177it [00:01, 95.06it/s]\u001b[A\n",
      "Train Epoch: 0 [2880/1000000 (0%)]\tLoss: 0.0134: : 177it [00:01, 93.03it/s]\u001b[A\n",
      "Train Epoch: 0 [2880/1000000 (0%)]\tLoss: 0.0134: : 187it [00:01, 95.27it/s]\u001b[A\n",
      "Train Epoch: 0 [3040/1000000 (0%)]\tLoss: 0.0146: : 187it [00:02, 93.11it/s]\u001b[A\n",
      "Train Epoch: 0 [3040/1000000 (0%)]\tLoss: 0.0146: : 197it [00:02, 95.12it/s]\u001b[A\n",
      "Train Epoch: 0 [3200/1000000 (0%)]\tLoss: 0.0121: : 197it [00:02, 93.25it/s]\u001b[A\n",
      "Train Epoch: 0 [3200/1000000 (0%)]\tLoss: 0.0121: : 207it [00:02, 95.25it/s]"
     ]
    }
   ],
   "source": [
    "hdnt_part = HardNetPart()\n",
    "hdnt_part.cuda()\n",
    "AffNetPix.cuda()\n",
    "descriptor.cuda()\n",
    "detector.cuda()\n",
    "#optimizer1 = torch.optim.Adam(hdnt_part.parameters(), lr = 0.0001)\n",
    "optimizer1 = create_optimizer(hdnt_part, 0.0001)\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    train(train_loader,test_loader, detector, descriptor, hdnt_part, optimizer1, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.AffNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid_original_image  = F.affine_grid(out_a_aff, patches.size())<br>\n",
    "transformed_patches = F.grid_sample(patches, grid_original_image))<br>\n",
    "transformed_descriptors = descriptor(transformed_patches).detach())<br>\n",
    "            \n",
    "out_a_aff выдает чуть не тот формат, что нравится F.affine_grid\n",
    "чтобы использовать out_a_aff, нужно использовать extract_patches\n",
    "как в трейне аффнета тут\n",
    "https://github.com/ducha-aiki/affnet/blob/master/train_AffNet_test_on_graffity.py#L217\n",
    "Понял\n",
    "\n",
    "вообще перед тренировкой и вообще использованием spatial transformers советую поиграться просто с картинкой - чтобы оно на вид правильно все экстрактило"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
